rule all:
    input:
        config['out']['outdir']+"/breedSample_all/breedSample_all.pdf",
        config['out']['outdir']+"/breedSample_all/boots/breedSample_all_boots.pdf",
        config['out']['outdir']+"/breedSample_breedSpecific/breedSample_breedSpecific.pdf",
        config['out']['outdir']+"/breedPlusMissingSample_breedSpecific/breedPlusMissingSample_breedSpecific.pdf"

# list of 12 breeds included in WES breed prediction
breedlist = ['Shih Tzu','Schnauzer','Golden Retriever','Rottweiler','Greyhound','Maltese','Yorkshire Terrier','Boxer','Poodle','Cocker Spaniel', "Labrador Retriever", "Boston Terrier"]

# create index files for individual VCFs
rule create_gz_index:
    input:
        vcf="{file}"
    output:
        rename=temp("{file}rename_samples_vcf.txt"),
        reheader_gz="{file}.reheader.gz",
        index="{file}.reheader.gz.csi"
    params:
        reheader="{file}.reheader"
    threads:
        1
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        rm -f {input.vcf}".gz" {input.vcf}".gz.csi"
        sample_name=`basename {input.vcf} "_rg_added_sorted_dedupped_removed.realigned.bam.filter.vcf"`
        echo "${{sample_name}}" > {output.rename}
        bcftools reheader -s {output.rename} --threads {threads} {input.vcf} | awk '{{ if($1 ~ /##/ || NF==10)print}}' > {params.reheader}
        bgzip --force --threads {threads} {params.reheader}
        bcftools index --threads {threads} {output.reheader_gz}
        echo "Compressed and indexed {input.vcf}"
        '''


# merge individual VCFs
rule merge_vcfs:
    input:
        vcf=expand("{file}.reheader.gz", file=config['in']['vcffiles']),
        index=expand("{file}.reheader.gz.csi", file=config['in']['vcffiles'])
    output:
        merged_vcf=config['out']['outdir']+"/all_merged_germline_variants.vcf.gz"
    threads:
        config['resources']['threads']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools merge --threads {threads} -Oz \
            --force-samples --missing-to-ref \
            -o {output.merged_vcf} \
            {input.vcf}
        '''

# initial filtering, only include PASS variants, only include SNPs
rule filter_merged_vcf:
    input:
        merged_vcf=config['out']['outdir']+"/all_merged_germline_variants.vcf.gz"
    output:
        filtered_vcf=config['out']['outdir']+"/all_merged_germline_variants_SNP.vcf.gz"
    threads:
        config['resources']['threads']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools view --threads {threads} -Oz \
            -f PASS --types snps \
            -o {output.filtered_vcf} \
            {input.merged_vcf}
        '''

# create samples list for breed samples and breed+missing samples for subsequent sample selection
# sampleSet = ['breedSample', 'breedPlusMissingSample']
# sample list inferred from metatable, possibly contains duplicates
rule create_sample_list:
    input:
        config['in']['metatable']
    output:
        breedSample=config['out']['outdir']+"/breedSample.list",
        breedPlusMissing=config['out']['outdir']+"/breedPlusMissingSample.list"
    params:
        breeds = breedlist
    run:
        import csv
        out1 = open(output['breedSample'], "w")
        out2 = open(output['breedPlusMissing'], "w")

        with open(input[0], 'r') as file:
            reader = csv.reader(file, delimiter=',', quotechar='"')
            header = next(reader)
            breed_idx = header.index('Breed_info')
            sample_idx = header.index('Sample_ID')
            status_idx = header.index('Status')
            qc_idx = header.index('Reason_to_exclude')
            for row in reader:
                if 'Normal' in row[status_idx] and 'Pass QC' in row[qc_idx]:
                    if row[breed_idx] in params['breeds']:
                        out1.write(row[sample_idx]+"\n")
                        out2.write(row[sample_idx]+"\n")
                    elif "No breed provided" in row[breed_idx]:
                        out2.write(row[sample_idx]+"\n")
        
        out1.close()
        out2.close()

# select samples from merged VCF, duplicated samples are removed in this step
rule subset_samples:
    input:
        filtered_vcf=config['out']['outdir']+"/all_merged_germline_variants_SNP.vcf.gz",
        breedSample=config['out']['outdir']+"/{sampleSet}.list"
    output:
        breedSample_vcf=config['out']['outdir']+"/{sampleSet}_all_merged_germline_variants_SNP.vcf.gz",
        breedSample_idx=config['out']['outdir']+"/{sampleSet}_all_merged_germline_variants_SNP.vcf.gz.csi"
    threads:
        config['resources']['threads']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools view --threads {threads} -O z \
            -S <(sort -u {input.breedSample}) \
            --force-samples \
            -o {output.breedSample_vcf} \
            {input.filtered_vcf}
        bcftools index {output.breedSample_vcf}
        '''

# reformat breed specific variants previously identified by breed predition pipeline
# scripts/breed_prediction/combine_MAF.sh
rule prepare_breedSpecific_sites:
    input:
        config['in']['breedSpecific']
    output:
        config['out']['outdir']+"/breedSpecific_sites.tab"
    shell:
        '''
        awk 'BEGIN{{OFS="\\t"}} {{if ($2 !~ /Locus/) {{split($2,POS,":"); print POS[1],POS[2];}} }}' \
            {input} > {output}
        '''

# sampleSet = ['breedSample', 'breedPlusMissingSample']
# regionSet = ['breedSpecific', 'all' ,'genes']
# select breed specific variants from merged VCF
rule subset_breedSpecific:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_all_merged_germline_variants_SNP.vcf.gz",
        idx=config['out']['outdir']+"/{sampleSet}_all_merged_germline_variants_SNP.vcf.gz.csi",
        sites=config['out']['outdir']+"/breedSpecific_sites.tab"
    output:
        vcf=config['out']['outdir']+"/{sampleSet}_breedSpecific_merged_germline_variants_SNP.vcf.gz",
        idx=config['out']['outdir']+"/{sampleSet}_breedSpecific_merged_germline_variants_SNP.vcf.gz.csi"
    threads:
        config['resources']['threads']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools view --threads {threads} -O z \
            -R {input.sites} \
            --force-samples \
            -o {output.vcf} \
            {input.vcf}
        bcftools index {output.vcf}
        '''

# reformat DepthOfCoverage results list, 
# remove duplicated samples to match samples in vcf file
rule create_depth_file_list:
    input:
        sampleSet=config['out']['outdir']+"/{sampleSet}.list",
        vcffilelist=config['in']['vcffilelist']
    output:
        depthfilelist=config['out']['outdir']+"/{sampleSet}_all_depth_file_list.txt"
    threads:
        1
    shell:
        '''
        # samples in order
        grep -f <(sort -u {input.sampleSet}) {input.vcffilelist} |  awk 'NR==FNR {{ line[$1] = $0; next }} $1 in line {{ print line[$1] }}' - <(sort -u {input.sampleSet}) | cut -f3 > {output.depthfilelist}
        '''

# create a list of depthofcoverage positions from the first depth file
# in WES pipeline, positions for DepthOfCoverage output should be identical
rule depth_position_list:
    input:
        depthfilelist=config['out']['outdir']+"/{sampleSet}_all_depth_file_list.txt"
    output:
        depthpositionlist=config['out']['outdir']+"/{sampleSet}_depth_position_list.txt",
        depthpositiontab=config['out']['outdir']+"/{sampleSet}_depth_position_list.tab"
    threads:
        1
    shell:
        '''
        # get depth position list from first depth file
        file=`head -1 {input.depthfilelist}`
        sed '1d' ${{file}} | cut -f1 > {output.depthpositionlist}
        awk 'BEGIN{{FS=":";OFS="\\t"}}{{print $1,$2}}' {output.depthpositionlist} > {output.depthpositiontab}
        '''

# further prune vcf to only include sites with depth info
# sites in merged VCF are different from interval of DepthOfCoverage
rule subset_depth_sites:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP.vcf.gz",
        depthpositiontab=config['out']['outdir']+"/{sampleSet}_depth_position_list.tab"
    output:
        out_vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_depth.vcf.gz",
        # out_index=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_depth.vcf.gz.csi"
    threads:
        config['resources']['threads']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools view --threads {threads} -O z \
            -R {input.depthpositiontab} \
            -o {output.out_vcf} \
            {input.vcf}
        # bcftools index {output.out_vcf}
        '''

# get list of variants from pruned vcf, just make sure the order of variants is consistent with depth matrix
rule create_variant_list:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_depth.vcf.gz",
    output:
        variantlist=config['out']['outdir']+"/{sampleSet}_{regionSet}_depth_variant_list.txt"
    threads:
        1
    shell:
        '''
        # get varaint sites in order
        zcat {input.vcf} |\
            awk 'BEGIN{{OFS=":"}}/^[^#]/{{print $1,$2}}' > {output.variantlist}
        '''

# create depth matrix for phylogenetic analysis
rule create_depth_matrix:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_depth.vcf.gz",
        variantlist=config['out']['outdir']+"/{sampleSet}_{regionSet}_depth_variant_list.txt",
        depthfilelist=config['out']['outdir']+"/{sampleSet}_all_depth_file_list.txt"
    output:
        tmp_depth=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}_tmp_DepthofCoverage_CDS.txt"),
        tmp_leftalign=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}_tmp_DepthofCoverage_CDS_leftalign.txt"),
        depthmatrix=config['out']['outdir']+"/{sampleSet}_{regionSet}_germline_depth_matrix_for_phylo.txt",
        tmp_matrix=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}_tmp.txt"),
        tmp_variantlist=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}_depth_variant_list_tmp.txt")
    threads:
        1
    shell:
        '''
        # select variant sites for depth info in order
        awk '{{print $0","}}' {input.variantlist} > {output.tmp_variantlist}
        while read file; do
            # sort depth file to fit the variant sites order, fill missing sites
            awk 'BEGIN{{OFS=","}}{{print $1,$2}}' ${{file}} | grep -f {output.tmp_variantlist} | cut -f1,2 > {output.tmp_depth}
            join -a1 -e- -t "," -j 1 -o 0 2.2 <(sort {input.variantlist}) <(sort {output.tmp_depth}) | tr : , | sort -t, -k1.4,1V -k2,2n | sed 's/,/:/' > {output.tmp_leftalign}
            if [[ ! -e {output.depthmatrix} ]]; then
                cp {input.variantlist} {output.depthmatrix}
                join -t "," -j 1 {output.depthmatrix} {output.tmp_leftalign} > {output.tmp_matrix}
                # paste -d, {output.depthmatrix} {output.tmp_leftalign} > {output.tmp_matrix}
                cp {output.tmp_matrix} {output.depthmatrix}
            else
                join -t "," -j 1 {output.depthmatrix} {output.tmp_leftalign} > {output.tmp_matrix}
                cp {output.tmp_matrix} {output.depthmatrix}
            fi
        done < {input.depthfilelist}
        '''

# mask low coverage sites in vcf
rule mask_low_coverage:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_depth.vcf.gz",
        depthmatrix=config['out']['outdir']+"/{sampleSet}_{regionSet}_germline_depth_matrix_for_phylo.txt"
    output:
        maksed_vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked.vcf.gz",
        maksed_idx=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked.vcf.gz.csi"
    params:
        mask_vcf=config['scripts']['mask_vcf'],
        minCoverage=10
    threads:
        config['resources']['threads']
    resources:
        mem=config['resources']['mem']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        python {params.mask_vcf} \
            {input.depthmatrix} \
            {input.vcf} \
            {output.maksed_vcf} \
            {params.minCoverage} \
            {threads} \
            {resources.mem}
        bcftools index {output.maksed_vcf}
        '''

# reformat known somatic mutation list
rule prepare_somatic_sites:
    input:
        config['in']['somaticMutation']
    output:
        config['out']['outdir']+"/somaticMutation_sites.tab"
    shell:
        '''
        awk 'BEGIN{{OFS="\\t"}} {{if ($1 !~ /chrom/) {{print $1,$2;}} }}' \
            {input} > {output}
        '''

# remove known somatic mutations from vcf
rule remove_somatic:
    input:
        maksed_vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked.vcf.gz",
        maksed_idx=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked.vcf.gz.csi",
        somatic=config['out']['outdir']+"/somaticMutation_sites.tab"
    output:
        cleaned_vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned.vcf.gz"
    threads:
        1
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools view --threads {threads} -O z \
            -T ^{input.somatic} \
            -o {output.cleaned_vcf} \
            {input.maksed_vcf}
        '''

# rule mhc2genes:
#     input:
#         cleaned_vcf=config['out']['outdir']+"/{sampleSet}_all_merged_germline_variants_SNP_masked.vcf.gz",
#         mhc2regions=config['in']['mhc2regions']
#     output:
#         region_vcf=config['out']['outdir']+"/{sampleSet}_mhc2_merged_germline_variants_SNP_masked_cleaned.vcf.gz"
#     threads:
#         1
#     conda:
#         config['envs']['phylogenetics']
#     shell:
#         '''
#         bcftools index -f {input.cleaned_vcf}
#         bcftools view --threads {threads} -O z \
#             -R {input.mhc2regions} \
#             -o {output.region_vcf} \
#             {input.cleaned_vcf}
#         '''

# generate distance matrix
rule distance_matrix:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned.vcf.gz"
    output:
        matrix=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.gz",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.id"
    threads:
        config['resources']['threads']
    resources:
        mem=config['resources']['mem']
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        mem_mb=`echo {resources.mem} | sed 's/G/000/g'`
        prefix=`echo {output.matrix} | sed 's/.mdist.gz//g'`
        plink --threads {threads} --memory ${{mem_mb}} --seed 123 --dog \
            --distance gz square '1-ibs' \
            --vcf {input.vcf} \
            --out ${{prefix}}
        '''

# reformat distance matrix to phylip format
rule mdist2phydist:
    input:
        matrix=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.gz",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.id",
        script=config['project_dir']+"/scripts/phylogenetic/mdist2phydist.py"
    output:
        phydist=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.phylip"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        python {input.script} \
            --mdist {input.matrix} \
            --id {input.distid} \
            --phydist {output.phydist} 
        '''

# reconstruct NJ tree with phylip,
# bootstrap is not implemented here
rule phylip_neighbor:
    input:
        phydist=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.phylip"
    output:
        tree=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}.nwk",
        command=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}.nwk.command")
    log:
        config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}.log"
    params:
        outdir=config['out']['outdir']+"/{sampleSet}_{regionSet}",
        outfile=config['out']['outdir']+"/{sampleSet}_{regionSet}/outfile",
        outtree=config['out']['outdir']+"/{sampleSet}_{regionSet}/outtree"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        cd {params.outdir}
        echo "{input.phydist}" > {output.command}
        echo "Y" >> {output.command}
        rm -f {params.outfile} {params.outtree}
        neighbor < {output.command} > {log}
        mv {params.outtree} {output.tree}
        cat {params.outfile} >> {log}
        rm {params.outfile}
        '''

# visualize NJ tree
rule phylip_nwk_vis:
    input:
        tree=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}.nwk",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.id",
        metatable=config['in']['metatable'],
        script=config['project_dir']+"/scripts/phylogenetic/phylip_nwk_vis.R"
    output:
        figure=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}.pdf"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        Rscript --vanilla {input.script} \
            {wildcards.sampleSet}_{wildcards.regionSet} \
            {input.tree} \
            {input.distid} \
            {input.metatable} \
            {output.figure}
        '''

##### implement bootstrapping #####
# get sample names from vcf
rule sample_id:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned.vcf.gz"
    output:
        samplelist=config['out']['outdir']+"/{sampleSet}_{regionSet}/samplelist.txt",
        sampleid=config['out']['outdir']+"/{sampleSet}_{regionSet}/sampleid.txt",
        list2id=config['out']['outdir']+"/{sampleSet}_{regionSet}/list2id.txt"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools query -l {input.vcf} > {output.samplelist}
        awk '{{print NR}}' {output.samplelist} > {output.sampleid}
        awk '{{print NR,$1}}' {output.samplelist} > {output.list2id}
        '''

# rename samples in vcf to 1,2,3...
rule rename_samples_vcf:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned.vcf.gz",
        sampleid=config['out']['outdir']+"/{sampleSet}_{regionSet}/sampleid.txt"
    output:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned_renamed.vcf.gz"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools reheader -s {input.sampleid} {input.vcf} -o {output.vcf}
        '''

# get the number of variants in vcf
rule get_variant_counts:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned_renamed.vcf.gz"
    output:
        config['out']['outdir']+"/{sampleSet}_{regionSet}/variant_counts.txt"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        bcftools index -f {input.vcf}
        bcftools index -n {input.vcf} > {output}
        '''

# randomly select 10% of variants for 100 times
rule resample_variants:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}_merged_germline_variants_SNP_masked_cleaned_renamed.vcf.gz",
        variant_counts=config['out']['outdir']+"/{sampleSet}_{regionSet}/variant_counts.txt"
    output:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.vcf.gz"
    params:
        seed=lambda wildcards: wildcards.rep,
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.vcf",
        variant_lines=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/variant_lines.vcf"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        # gatk IndexFeatureFile -I {input.vcf}
        # gatk SelectVariants \
        #     --variant {input.vcf} \
        #     --select-random-fraction 0.1 \
        #     --output {output.vcf} 
        subcount=$(($(cat {input.variant_counts}) / 10))
        bcftools view --header-only {input.vcf} > {params.vcf}
        bcftools view --no-header {input.vcf} | awk -v seed={params.seed} 'BEGIN{{srand(seed);}}{{printf("%f\\t%s\\n",rand(),$0);}}' | sort -t $'\\t' -k1,1g | cut -f2- > {params.variant_lines}
        head -n${{subcount}} {params.variant_lines} | sort >> {params.vcf}
        gzip {params.vcf}
        rm {params.variant_lines}
        '''

# generate distance matrix for each resampling
rule distance_matrix_boots:
    input:
        vcf=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.vcf.gz"
    output:
        matrix=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.mdist.gz",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.mdist.id"
    threads:
        1
    resources:
        mem="2G"
    conda:
        config['envs']['phylogenetics']
    params:
        seed=lambda wildcards: wildcards.rep
    shell:
        '''
        mem_mb=`echo {resources.mem} | sed 's/G/000/g'`
        prefix=`echo {output.matrix} | sed 's/.mdist.gz//g'`
        plink --threads {threads} --memory ${{mem_mb}} --seed {params.seed} --dog \
            --distance gz square '1-ibs' \
            --vcf {input.vcf} \
            --out ${{prefix}}
        '''

# convert distance matrix to phylip format for each resampling
rule mdist2phydist_boots:
    input:
        matrix=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.mdist.gz",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.mdist.id",
        script=config['project_dir']+"/scripts/phylogenetic/mdist2phydist.py"
    output:
        phydist=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.phylip"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        python {input.script} \
            --mdist {input.matrix} \
            --id {input.distid} \
            --phydist {output.phydist} \
            --keeptaxa
        '''

# reconstruct NJ tree for each resampling
rule phylip_neighbor_boots:
    input:
        phydist=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.phylip"
    output:
        tree=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.nwk",
        command=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled.nwk.command")
    log:
        config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/downsampled_{rep}.log"
    params:
        outdir=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}",
        outfile=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/outfile",
        outtree=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{rep}/outtree"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        cd {params.outdir}
        echo "{input.phydist}" > {output.command}
        echo "Y" >> {output.command}
        rm -f {params.outfile} {params.outtree}
        neighbor < {output.command} > {log}
        mv {params.outtree} {output.tree}
        cat {params.outfile} >> {log}
        rm {params.outfile}
        '''

# generate consensus tree from 100 resamplings
rule phylip_consense:
    input:
        expand(config['out']['outdir']+"/{{sampleSet}}_{{regionSet}}/boots/{rep}/downsampled.nwk", rep = range(1,101))
    output:
        trees=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}_trees.nwk",
        consensus=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}_consensus.nwk",
        command=temp(config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}.nwk.command")
    conda:
        config['envs']['phylogenetics']
    params:
        outdir=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/",
        outfile=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/outfile",
        outtree=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/outtree"
    log:
        config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}_consensus.log"
    shell:
        '''
        cd {params.outdir}
        cat {input} > {output.trees}
        echo "{output.trees}" > {output.command}
        echo "Y" >> {output.command}
        consense < {output.command} > {log}
        mv {params.outtree} {output.consensus}
        cat {params.outfile} >> {log}
        rm {params.outfile}
        '''

# visualize consensus tree
rule phylip_nwk_vis_boots:
    input:
        tree=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}_consensus.nwk",
        distid=config['out']['outdir']+"/{sampleSet}_{regionSet}/{sampleSet}_{regionSet}_merged_germline_variants_SNP.mdist.id",
        metatable=config['in']['metatable'],
        script=config['project_dir']+"/scripts/phylogenetic/phylip_nwk_vis.R"
    output:
        figure=config['out']['outdir']+"/{sampleSet}_{regionSet}/boots/{sampleSet}_{regionSet}_boots.pdf"
    conda:
        config['envs']['phylogenetics']
    shell:
        '''
        Rscript --vanilla {input.script} \
            {wildcards.sampleSet}_{wildcards.regionSet} \
            {input.tree} \
            {input.distid} \
            {input.metatable} \
            {output.figure}
        '''